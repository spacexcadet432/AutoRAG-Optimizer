# AutoRAG-Optimizer
 A configurable Retrieval-Augmented Generation (RAG) experimentation framework for systematically evaluating chunking strategies, retrieval depth, and prompt styles using FAISS and embedding-based metrics.


# AutoRAG-Optimizer

A configurable Retrieval-Augmented Generation (RAG) experimentation framework for systematically evaluating chunking strategies, retrieval depth, and prompt styles using FAISS and embedding-based metrics.

---

## ğŸš€ Problem Statement

Most RAG systems are tuned manually through trial and error.  
There is no structured way to evaluate how chunk size, overlap, retrieval depth (top-k), and prompt strategies impact performance.

This project builds a reproducible evaluation framework to benchmark RAG configurations quantitatively.

---

## ğŸ§  What This Project Does

- Implements RAG from scratch (no LangChain)
- Uses FAISS for vector similarity search
- Supports configurable:
  - Chunk size
  - Chunk overlap
  - Retrieval depth (top-k)
  - Prompt strategy (basic vs strict grounding)
- Performs grid search over multiple configurations
- Evaluates performance using:
  - Embedding-based answer similarity
  - Retrieval coverage metric
  - Chunk count (computational cost proxy)
- Automatically identifies the best-performing configuration

---

## ğŸ— Architecture


Document (.txt)
â†“
Chunking (overlap-aware)
â†“
OpenAI Embeddings
â†“
FAISS Vector Index
â†“
Top-k Retrieval
â†“
Prompted Generation
â†“
Evaluation:
- Answer Similarity
- Retrieval Coverage
- Chunk Count


---

## ğŸ“Š Evaluation Metrics

### 1ï¸âƒ£ Average Similarity
Semantic similarity between generated answer and expected answer using embedding cosine similarity.

### 2ï¸âƒ£ Retrieval Accuracy
Checks whether the expected answer appears inside retrieved chunks.

### 3ï¸âƒ£ Number of Chunks
Tracks how chunk size affects index size and computational cost.

---

## ğŸ”¬ Example Experiment

Grid search over:

- chunk_size: [300, 500]
- overlap: [50, 100]
- top_k: [3, 5]
- prompt_style: ["basic", "strict"]

The framework automatically ranks configurations by:

1. Average similarity  
2. Retrieval accuracy  

---

## ğŸ›  Tech Stack

- Python
- FAISS (vector search)
- OpenAI Embeddings API
- NumPy
- python-dotenv

---

## ğŸ“‚ Project Structure


auto-rag-optimizer/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ documents/
â”‚ â””â”€â”€ test_questions.json
â”‚
â”œâ”€â”€ rag/
â”‚ â”œâ”€â”€ chunker.py
â”‚ â”œâ”€â”€ embedder.py
â”‚ â”œâ”€â”€ retriever.py
â”‚ â”œâ”€â”€ generator.py
â”‚ â””â”€â”€ pipeline.py
â”‚
â”œâ”€â”€ optimizer/
â”‚ â”œâ”€â”€ config_generator.py
â”‚ â”œâ”€â”€ evaluator.py
â”‚ â””â”€â”€ experiment_runner.py
â”‚
â”œâ”€â”€ utils/
â”‚ â””â”€â”€ metrics.py
â”‚
â””â”€â”€ README.md


---

## â–¶ï¸ How To Run

1. Install dependencies:

```bash
pip install -r requirements.txt

Add your OpenAI API key in a .env file:

OPENAI_API_KEY=your_key_here

Run experiments:

python -m optimizer.experiment_runner
